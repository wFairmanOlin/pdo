{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3: Multi-objective Optimization\n",
    "\n",
    "---\n",
    "\n",
    "*Purpose*: Many real engineering problems have multiple objectives. In this case optimization is greatly complicated. In this assignment you'll learn about non-dominance, the Pareto frontier, and how to use optimization to identify a frontier of efficient designs.\n",
    "\n",
    "*Learning Objectives*: By working through this assignment, you will \n",
    "- use pipe-enabled Grama verbs to write *readable* code\n",
    "- use `gr.eval_min()` to solve optimization problems with Grama models\n",
    "- learn the basic concepts of multi-objective optimization: non-dominance and the Pareto frontier\n",
    "- use scalarization techniques and repeated optimization to construct a Pareto frontier\n",
    "\n",
    "*Reading*:\n",
    "- Kochenderfer and Wheeler, Ch 12.1, 12.2, 12.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Checklist\n",
    "\n",
    "1. [?] Make sure you have answered all questions. These are marked with a **qX.Y**\n",
    "1. [?] Make sure you complete the Project Task at the end of the assignment. These will scaffold your project progress during the semester.\n",
    "1. [?] Make sure your notebook passes all `assert()` statements. You will not get full credit for the assignment if a single `assert()` fails.\n",
    "1. [?] Make sure your notebook runs: `Kernel > Restart kernel and run all cells...`\n",
    "1. [?] Upload your notebook to Canvas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Rubric\n",
    "\n",
    "Every assignment is worth 10 points; it is not possible to receive less than 0 points. For each question (qX.Y) on a given assignment, the following grading rubric will be applied. For every NI that you receive, one point will be subtracted from your assignment total. For reference, to receive an A- in this class, you will need an average of 9 points across your 5 best assignments, meaning you need to have at most one mistake on your final submission for 5 assignments. To achieve this, you should take advantage of both the Draft and Final submission deadlines.\n",
    "\n",
    "| Category     | Needs Improvement (NI)                     | Satisfactory (S)                       |\n",
    "|--------------|--------------------------------------------|----------------------------------------|\n",
    "| Effort       | qX.Y left unattempted                      | qX.Y attempted                         |\n",
    "| Assertions   | Code does not pass an `assert()`           | All `assert()`s pass, or no assertions |\n",
    "| Observations | Any point under *observe* left unattempted | All *observe*s attempted and correct,  |\n",
    "|              | Provided an incorrect observation          | or no *observe*s for that q            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S1: Sinews and Pipes\n",
    "\n",
    "---\n",
    "\n",
    "First, we'll learn some more useful Grama stuff: Pipelines, the data pronoun, and sinew plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grama as gr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pandas as pd\n",
    "\n",
    "from grama.models import make_cantilever_beam\n",
    "\n",
    "# Define the data pronoun as DF\n",
    "DF = gr.Intention()\n",
    "\n",
    "# Set figure options\n",
    "plt.rcParams['figure.figsize'] = [6, 4]\n",
    "plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinew Plots\n",
    "\n",
    "We'll often have to make sense of high-dimensional functions. One of the most useful things we can do as a \"first check\" of a model is to construct *sinew plots*. Let's look at a very simple function to get some intuition: Here's the function\n",
    "\n",
    "$$f(x, a) = a \\exp(x)$$\n",
    "\n",
    "And here's an implementation in Grama:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "md_simple = (\n",
    "    gr.Model()\n",
    "    >> gr.cp_vec_function(\n",
    "        fun=lambda df: gr.df_make(\n",
    "            f=df.a * np.exp(df.x)\n",
    "        ),\n",
    "        var=[\"x\", \"a\"],\n",
    "        out=[\"f\"]\n",
    "    )\n",
    "    >> gr.cp_bounds(\n",
    "        a=(-1, +1),\n",
    "        x=(-1, +1),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know this function is going to be *linear* in its argument $a$, and *exponential* in its argument $x$. So if we were to *hold all other variables constant* and vary one input at a time, we could take a look at the effect of just one input. This is what a sinew plot does.\n",
    "\n",
    "*Aside*: Some folks call these [ceteris paribus plots](https://ema.drwhy.ai/ceterisParibus.html), where *ceteris paribus* is latin for *all else equal*. I'm not a fan of dead languages, so I just call them sinews.\n",
    "\n",
    "We can get a better sense by visualizing which points the sinew operation will evaluate:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.plot_auto(\n",
    "    gr.eval_sinews(\n",
    "        md_simple,\n",
    "        df_det=\"swp\",\n",
    "        skip=True,\n",
    "        n_density=20,\n",
    "        n_sweeps=3,\n",
    "        seed=101,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `skip=True` tells Grama not to evaluate the functions, but to just give us the points it plans to evaluate. Calling `plot_auto()` on the result uses some metadata to construct an automatic plot that helps visualize the results.\n",
    "\n",
    "This plot shows what `eval_sinews()` does: We pick random points in the `a, x` plane (`n_sweeps` of them) and sweep parallel to the axes of `a` and `x`. This gives us a set of points to evaluate. If we get rid of the `skip` keyword, Grama will evaluate the function and show us the response against each input variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.plot_auto(\n",
    "    gr.eval_sinews(\n",
    "        md_simple,\n",
    "        df_det=\"swp\",\n",
    "#         skip=True,\n",
    "        n_density=20,\n",
    "        n_sweeps=3,\n",
    "        seed=101,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that indeed, the function is linear in `a` and exponential in `x`. However, we can also see how *other* variables modulate those effects. We can see that, depending on the value of `a`, the function could either grow positively or negatively in `x`. And for the linear growth in `a`, its slope is determined by `x`.\n",
    "\n",
    "For a modest number of variables---say fewer than 5---a sinew plot can help us make sense of our model. With more variables, it's hard to make sense of all the complex interactions between variables in the function. If you have a model with many more variables, you can ask me about *dimension reduction* techniques sometime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Grama is designed around *analysis pipelines*, in the style of [functional programming](https://en.wikipedia.org/wiki/Functional_programming). In a pipeline one successively applies functions; an advantage of this functional style is that one can easily read individual steps corresponding to each function.\n",
    "\n",
    "To help make functional code readable, Grama functions overload the shift operator `>>` to serve as a *pipe*. The pipe takes its left hand side (LHS), and inserts the LHS as the first argument to the function on its right.\n",
    "\n",
    "**Important concept**: When you see `x >> fun(*args)`, you should think of this as equivalent to `fun(x, *args)`.\n",
    "\n",
    "This pipe operator lets us *transform* code to make it more readable. For instance, the following code is borderline unintelligible:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: No need to edit, just run and inspect\n",
    "gr.plot_auto(\n",
    "    gr.eval_sinews(\n",
    "        make_cantilever_beam(),\n",
    "        df_det=\"nom\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is hard to read in part because we need to start from the *inside* of the functions to understand where the computation starts. This is made considerably more readable by introducing *intermediate variables*: The following code does *exactly* the same thing as the code above, but assigns intermediate variables `md_beam` and `df_sweep`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: No need to edit, just run and inspect\n",
    "md_beam = make_cantilever_beam()\n",
    "df_sweep = gr.eval_sinews(md_beam, df_det=\"nom\")\n",
    "gr.plot_auto(df_sweep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipe operator `>>` allows us to remove the intermediate variables, which can be useful when we're carrying out many operations. Let's rewrite the code one last time using pipes. Note that the use of parentheses around the code allows us to use newlines without needing a line continuation character `\\`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: No need to edit, just run and inspect\n",
    "(\n",
    "    md_beam\n",
    "    >> gr.ev_sinews(df_det=\"nom\")\n",
    "    >> gr.pt_auto()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the *prefix* for the grama functions changed; the pipe-enabled version of the function has a two-letter prefix. The prefixes also denote the inputs and outputs of the function:\n",
    "\n",
    "| Prefix (Short) | Input | Output |\n",
    "|---|---|---\n",
    "| `eval` (`ev`) | Model | DataFrame |\n",
    "| `fit` (`ft`) | DataFrame | Model |\n",
    "| `comp` (`cp`) | Model | Model |\n",
    "| `tran` (`tf`) | DataFrame | DataFrame |\n",
    "| `plot` (`pt`) | DataFrame | (Plot) |\n",
    "\n",
    "You don't need to memorize this table; you can come back and reference it (or better yet, save the relevant [documentation page](https://py-grama.readthedocs.io/en/latest/source/language.html#verbs)). Just know that you can tell a lot about a (Grama) function by looking at its prefix; that alone tells you what type of input it will take, and what type of output it will provide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q1.1 Rewrite this code with pipes\n",
    "\n",
    "Rewrite the following code using the pipe `>>` operator.\n",
    "\n",
    "*Hint*: If you do this correctly, you should get an identical set of histograms as the existing code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Re-write the following code using pipes `>>`\n",
    "###\n",
    "\n",
    "# NOTE: No need to edit; this is the code you'll translate\n",
    "md_beam = make_cantilever_beam()\n",
    "df_mc = gr.eval_monte_carlo(md_beam, n=1e3, df_det=\"nom\")\n",
    "gr.plot_auto(df_mc)\n",
    "\n",
    "# TASK: Re-write this code with pipes `>>` to \n",
    "#       eliminate intermediate variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data pronoun\n",
    "\n",
    "Grama also provides a function `Intention()`, which allows us to define a *data pronoun*. The setup code above assigns this with `DF = gr.Intention()`, which means we can use `DF` as a data pronoun.\n",
    "\n",
    "### Filtering\n",
    "\n",
    "The use of this shorthand allows us to simplify code. For instance, we can re-write the following filter code:\n",
    "\n",
    "```\n",
    "df_longdataname[df_longdataname.x > df_longdataname.y]\n",
    "```\n",
    "\n",
    "Using a call to `gr.tf_filter()` using the data pronoun\n",
    "\n",
    "```\n",
    "df_longdataname >> gr.tf_filter(DF.x > DF.y)\n",
    "```\n",
    "\n",
    "### Mutating\n",
    "\n",
    "The use of the data pronoun also allows us to avoid assigning intermediate variables. If we wished to filter then modify data, we would need to do this in pure pandas with an intermediate DataFrame:\n",
    "\n",
    "```\n",
    "df_tmp = df_data[df_data.x > x_threshold]\n",
    "df_tmp.y = df_tmp.x ** 2\n",
    "```\n",
    "\n",
    "Using `gr.tf_mutate()` we can add / edit a column, and we can use this in a pipeline with a filter to compose the two operations.\n",
    "\n",
    "```\n",
    "(\n",
    "    df_data\n",
    "    >> gr.tf_filter(DF.x > x_threshold)\n",
    "    >> gr.tf_mutate(y=DF.x ** 2)\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q1.2 Use the data pronoun to filter\n",
    "\n",
    "Recreate the following pure-pandas filtering operation using a *single* call to `gr.tf_filter()` and the data pronoun `DF`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Recreate the following data manipulation using\n",
    "#       `gr.tf_filter()` and the data pronoun `DF`\n",
    "###\n",
    "\n",
    "# NOTE: No need to edit; this sets up a toy dataset\n",
    "df_foo = gr.df_make(\n",
    "    x=[0, 1, 2],\n",
    "    y=[2, 1, 0],\n",
    ")\n",
    "\n",
    "# NOTE: No need to edit; recreate this with a single filter\n",
    "df_tmp = df_foo.copy()\n",
    "df_tmp[\"d\"] = df_tmp.x**2 + df_tmp.y**2\n",
    "df_tmp[\"b\"] = df_tmp.d <= 2\n",
    "df_tmp = df_tmp[df_tmp.b]\n",
    "\n",
    "# TASK: Recreate the filtering operation above, but do so with a \n",
    "#       single call of gr.tf_filter() using the data pronoun DF\n",
    "# df_pipe = ???\n",
    "\n",
    "\n",
    "# NOTE: No need to edit; use to check your answer\n",
    "assert(df_pipe.equals(df_tmp[[\"x\", \"y\"]].reset_index(drop=True)))\n",
    "print(\"Success!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `eval_min()` and pipeline model adjustments\n",
    "\n",
    "Let's use these ideas to make \"last minute\" adjustments to a model before we use it in an optimization problem.\n",
    "\n",
    "First, let's introduce `gr.eval_min()`: This is a *wrapper* for `scipy.optimize.minimize` that allows us to set an objective and constraints based on model outputs. If we were not using `gr.eval_min()`, we would need to write our own wrapper functions to use a Grama model with `scipy.optimize.minimize`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: No need to edit; run and inspect\n",
    "md_base = (\n",
    "    gr.Model(\"Example model\")\n",
    "    >> gr.cp_vec_function(\n",
    "        fun=lambda df: gr.df_make(\n",
    "            u=df.x + df.y + 1,\n",
    "            v=df.x - df.y - 1,\n",
    "        ),\n",
    "        var=[\"x\", \"y\"],\n",
    "        out=[\"u\", \"v\"],\n",
    "    )\n",
    "    >> gr.cp_bounds(\n",
    "        x=(-1, +1),\n",
    "        y=(-1, +1),\n",
    "    )\n",
    ")\n",
    "\n",
    "# The following pipeline modifies a model and runs an optimization\n",
    "df_res_tmp = (\n",
    "    md_base\n",
    "    # Add another function to md_base, summarizing its outputs\n",
    "    >> gr.cp_function(\n",
    "        fun=lambda x: (0.5 * x[0] + 0.5 * x[1] - 1)**2,\n",
    "        var=[\"u\", \"v\"],\n",
    "        out=[\"obj\"],\n",
    "    )\n",
    "    # Run optimization on this new objective\n",
    "    >> gr.ev_min(out_min=\"obj\")\n",
    ")\n",
    "df_res_tmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this \"just in time\" adjustment to apply the scalarization idea you read about in Kochenderfer and Wheeler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q1.3 Optimize with `gr.eval_min()`\n",
    "\n",
    "Using `md_base` as a starting point, solve the following optimization problem:\n",
    "\n",
    "$$\\min\\, x^2 + y^2$$\n",
    "$$\\text{wrt.}\\, x, y$$\n",
    "$$\\text{s.t.}\\, u(x, y) \\leq 0$$\n",
    "$$\\text{s.t.}\\, v(x, y) \\leq 0$$\n",
    "\n",
    "Solve the optimization problem with `gr.eval_min()`.\n",
    "\n",
    "*Hint*: Remember to use `Shift + Tab` to read the documentation for `gr.eval_min()`!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Implement and solve the optimization problem above\n",
    "###\n",
    "\n",
    "df_q1_3 = (\n",
    "    md_base\n",
    "    # TASK: Compose the objective function\n",
    "    # TASK: Optimize with gr.ev_min()\n",
    ")\n",
    "\n",
    "\n",
    "# NOTE: No need to edit, use this to check your solution\n",
    "assert(abs(df_q1_3.x[0] + 0.5) < 1e-6)\n",
    "assert(abs(df_q1_3.y[0] + 0.5) < 1e-6)\n",
    "print(\"Success!\")\n",
    "df_q1_3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing; because vectorized Grama functions take DataFrames and return DataFrames, we can use pipelines *within* a Grama function. This allows us to do trivial things in a more complicated way (which is not compelling):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit; this demonstrates using pipeline tools\n",
    "# inside a Grama function\n",
    "md_silly = (\n",
    "    gr.Model()\n",
    "    >> gr.cp_vec_function(\n",
    "        fun=lambda df: gr.df_make(f=df.x)\n",
    "        >> gr.tf_mutate(g=DF.f + DF.y),\n",
    "        var=[\"x\", \"y\"],\n",
    "        out=[\"f\", \"g\"]\n",
    "    )\n",
    "    >> gr.cp_bounds(\n",
    "        x=(0, 1),\n",
    "        y=(0, 1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the use of pipeline-enabled data manipulation allows us to do *very sophisticated* data operations within a Grama model, which will enable important operations we'll see in NB4. The tricky thing to look out for is to *not* get the variables `df` and `DF` confused; if you run into issues with your code, keep a look out for swapping the two.\n",
    "\n",
    "We'll use these tools for combining model outputs in our efforts to tackle *multi-objective optimization* below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2: Multi-objective Optimization: The Pareto Frontier\n",
    "\n",
    "---\n",
    "\n",
    "To this point we've studied problems that have **just one** objective to optimize. However real engineering problems often have more than one objective. This situation is called *multi-objective optimization*, and it is *inherently* more complicated than single-objective optimization.\n",
    "\n",
    "The following figure illustrates two objective function axes, both of which we wish to maximize. We see a single candidate at the center of the diagram, and the quadrants about that candidate are labeled:\n",
    "\n",
    "<img src=\"./images/obj-multi.png\" width=\"400\">\n",
    "\n",
    "In multi-objective optimization we are *not* guaranteed to be able to order two candidates as inherently better or worse. Instead two candidates can be mutually *non-dominating*---this occurs when one candidate is superior along one or more axes, but the other candidate is superior along different axes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/obj-ex.png\" width=\"400\">\n",
    "\n",
    "The existence of non-dominated candidate pairs means *there is no unique optimum* in the multi-objective setting. Instead, there is a set of non-dominated candidates, as shown in the example above. This set is called the [Pareto frontier](https://en.wikipedia.org/wiki/Pareto_efficiency). \n",
    "\n",
    "The following code example illustrates a Pareto frontier with the aircraft dataset, highlighting those cases that have minimal body weight but carry maximal passengers. The function `gr.pareto_min()` is a helper function that computes Pareto-efficient points in a given dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: No need to edit; this illustrates a Pareto frontier\n",
    "# Load the data\n",
    "filename_aircraft = \"./data/wing_weight.csv\"\n",
    "df_aircraft = pd.read_csv(filename_aircraft)\n",
    "\n",
    "# Wrangle the dataset into a usable form\n",
    "# NOTE: If you want to learn how to do this, take Data Science!\n",
    "aircraft = df_aircraft.drop(\n",
    "    [\"Group\", \"Name\", \"Symbol\", \"Units\"],\n",
    "    axis=1\n",
    ").columns\n",
    "df_properties = (\n",
    "    df_aircraft\n",
    "    >> gr.tf_gather(\"aircraft\",  \"value\",  aircraft)\n",
    "    >> gr.tf_select(DF.Symbol, DF.aircraft, DF.value)\n",
    "    >> gr.tf_spread(\"Symbol\", \"value\", convert=True)\n",
    "    # Note that I use -N_p to maximize N_p\n",
    "    >> gr.tf_mutate(pareto=gr.pareto_min(DF.W_3, -DF.N_p))\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    df_properties[~df_properties.pareto][\"W_3\"], \n",
    "    df_properties[~df_properties.pareto][\"N_p\"], \n",
    "    \"k.\"\n",
    ")\n",
    "plt.plot(\n",
    "    df_properties[df_properties.pareto][\"W_3\"], \n",
    "    df_properties[df_properties.pareto][\"N_p\"], \n",
    "    \"r.\",\n",
    "    label=\"Pareto optimal\"\n",
    ")\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Body Weight (lbs)\")\n",
    "plt.ylabel(\"Passenger Capacity (persons)\")\n",
    "plt.legend(loc=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there might be two very different classes of aircraft in this dataset (I see two parallel lines of aircraft, which looks like two separate groups). Clearly the top group is more efficient at carrying passengers than the lower one, for fixed weight. However, the bottom group might be optimized for other criteria. You could take a closer look at the dataset to figure out more!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q2.1 Identify a Pareto Frontier\n",
    "\n",
    "The following is the [Binh and Korn](https://en.wikipedia.org/wiki/Test_functions_for_optimization#Test_functions_for_multi-objective_optimization) optimization problem.\n",
    "\n",
    "$$\\min\\, f_1, f_2$$\n",
    "$$\\text{wrt.}\\, x, y$$\n",
    "$$\\text{s.t.}\\, g_1 \\leq 0$$\n",
    "$$\\text{s.t.}\\, g_2 \\geq 0$$\n",
    "$$\\text{s.t.}\\, 0 \\leq x \\leq 5$$\n",
    "$$\\text{s.t.}\\, 0 \\leq y \\leq 3$$\n",
    "\n",
    "The following code generates data from this problem. Your task is to filter out only those points satisfying the constraints, and to identify the Pareto-optimal points. Answer the questions below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Filter the data that satisfy the constraints\n",
    "#       and add a column that identifies Pareto points\n",
    "###\n",
    "\n",
    "# NOTE: No need to edit, this sets up a model to optimize\n",
    "md_BK = (\n",
    "    gr.Model(\"Binh and Korn model\")\n",
    "    >> gr.cp_vec_function(\n",
    "        fun=lambda df: gr.df_make(\n",
    "            f1=4 * df.x**2 + 4 * df.y**2\n",
    "        ),\n",
    "        var=[\"x\", \"y\"],\n",
    "        out=[\"f1\"],\n",
    "    )\n",
    "    >> gr.cp_vec_function(\n",
    "        fun=lambda df: gr.df_make(\n",
    "            f2=(df.x - 5)**2 + (df.y - 5)**2\n",
    "        ),\n",
    "        var=[\"x\", \"y\"],\n",
    "        out=[\"f2\"],\n",
    "    )\n",
    "    >> gr.cp_vec_function(\n",
    "        fun=lambda df: gr.df_make(\n",
    "            g1_leq=(df.x - 5)**2 + df.y**2 - 25\n",
    "        ),\n",
    "        var=[\"x\", \"y\"],\n",
    "        out=[\"g1_leq\"],\n",
    "    )\n",
    "    >> gr.cp_vec_function(\n",
    "        fun=lambda df: gr.df_make(\n",
    "            g2_geq=(df.x - 8)**2 - (df.y + 3)**2 - 7.7\n",
    "        ),\n",
    "        var=[\"x\", \"y\"],\n",
    "        out=[\"g2_geq\"],\n",
    "    )\n",
    "    >> gr.cp_bounds(\n",
    "        x=(0, 5),\n",
    "        y=(0, 3),\n",
    "    )\n",
    ")\n",
    "# Generate samples uniformly across the domain\n",
    "df_random = (\n",
    "    md_BK\n",
    "    >> gr.cp_marginals(\n",
    "        x=dict(dist=\"uniform\", loc=0, scale=5),\n",
    "        y=dict(dist=\"uniform\", loc=0, scale=3),\n",
    "    )\n",
    "    >> gr.cp_copula_independence()\n",
    "    >> gr.ev_monte_carlo(n=1e2, df_det=\"nom\", seed=102)\n",
    ")\n",
    "\n",
    "# TASK: Filter the data, compute the Pareto points\n",
    "# df_BK = ???\n",
    "\n",
    "\n",
    "# NOTE: No need to edit; this visualizes your results\n",
    "plt.figure()\n",
    "plt.plot(df_BK.f1, df_BK.f2, \"k.\")\n",
    "plt.plot(\n",
    "    df_BK[df_BK.pareto].f1,\n",
    "    df_BK[df_BK.pareto].f2,\n",
    "    \"r.\",\n",
    "    label=\"Optimal, among samples\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"f1\")\n",
    "plt.ylabel(\"f2\")\n",
    "plt.legend(loc=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observe*:\n",
    "\n",
    "- What shape---roughly---does the Pareto frontier have?\n",
    "  - (Your response here)\n",
    "- Here we approximated the Pareto frontier using *random sampling*. Try increasing the `n` in `gr.ev_monte_carlo()` above; what happens when you increase `n`?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q2.2 Scalarization\n",
    "\n",
    "Use the weight method (Kochenderfer and Wheeler, Ch 12.3) to scalarize the objective $f_1, f_2$ and find a candidate near the \"middle\" of the Pareto frontier. Use `gr.ev_min()` to solve the optimization. Answer the question below.\n",
    "\n",
    "*Hint*: Remember that you can use `md_BK.printpretty()` to inspect the model to determine the names of its outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Scalarize the objectives f1, f2 with the weight method,\n",
    "#       solve the optimization problem to identify a point on the\n",
    "#       Pareto frontier\n",
    "###\n",
    "\n",
    "# df_BK_opt = ???\n",
    "\n",
    "\n",
    "# NOTE: No need to edit; this visualizes your results\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    df_BK[df_BK.pareto].f1,\n",
    "    df_BK[df_BK.pareto].f2,\n",
    "    \"k.\"\n",
    ")\n",
    "plt.plot(df_BK_opt.f1, df_BK_opt.f2, \"ro\")\n",
    "\n",
    "plt.xlabel(\"f1\")\n",
    "plt.ylabel(\"f2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observe*:\n",
    "\n",
    "- Your identified point (in red) should lie near the \"middle\" of the Pareto frontier. What weights $w_{f_1}, w_{f_2}$ did you choose to find this point?\n",
    "  - (Your response here)\n",
    "- What relationship does the vector $(w_{f_1}, w_{f_2})$ have with the Pareto frontier?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3: Case Study: Design of a Cantilever Beam\n",
    "\n",
    "---\n",
    "\n",
    "A [cantilever beam](https://en.wikipedia.org/wiki/Cantilever) is a fundamental structural element. Many engineering systems are cantilevers: airplane wings, bridges, microelectronic systems, and so on. When designing structural elements, we often have a fundamental tradeoff to consider: a heavier structure will tend to be safer but more expensive. \n",
    "\n",
    "In this case study we'll consider the sizing of a cantilever beam of predetermined length $l$ in terms of its width $w$ and thickness $t$. This beam is subject to predetermined horizontal and vertical tip load. We seek to minimize its volume $V$, but we also want to minimize its tip displacement $D$. The beam must be structurally sound, so we impose a constraint based on the maximum bending stress $\\sigma_y \\geq \\sigma_{\\text{max bend}}$.\n",
    "\n",
    "The following states our optimization problem in standard form:\n",
    "\n",
    "$$\\min\\, V, D$$\n",
    "$$\\text{wrt.}\\, w, t$$\n",
    "$$\\text{s.t.}\\, 4 \\leq w \\leq 30$$\n",
    "$$\\text{s.t.}\\, 4 \\leq t \\leq 30$$\n",
    "$$\\text{s.t.}\\, \\sigma_y - \\sigma_{\\text{applied}} \\geq 0$$\n",
    "\n",
    "Your first task will be to assemble a Grama model using the following functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: *Assume* values for these variable quantities\n",
    "#       we'll learn how to model this with uncertainty later!\n",
    "E_NOM = 1.45e6 # Modulus of elasticity (psi)\n",
    "H_NOM = 500    # Horizontal tip force (lbs)\n",
    "V_NOM = 1000   # Vertical tip force (lbs)\n",
    "Y_NOM = 2000   # Ultimate stress (psi)\n",
    "l_NOM = 200    # beam length (in)\n",
    "var_cantilever = [\"w\", \"t\"]\n",
    "\n",
    "# NOTE: No need to edit, this sets up the cantilever beam problem\n",
    "def fun_volume(df):\n",
    "    return gr.df_make(vol=df.w * df.t * l_NOM)\n",
    "out_volume = [\"vol\"]\n",
    "\n",
    "def fun_stress_diff(df):\n",
    "    return gr.df_make(\n",
    "        stress_diff=Y_NOM \n",
    "        - 6 * l_NOM * V_NOM / df.w / df.t**2 \n",
    "        - 6 * l_NOM * H_NOM / df.w**2 / df.t\n",
    "    )\n",
    "out_stress_diff = [\"stress_diff\"]\n",
    "\n",
    "def fun_disp(df):\n",
    "    return gr.df_make(\n",
    "        disp=np.float64(4) * l_NOM**3 / E_NOM / df.w / df.t * np.sqrt(\n",
    "        V_NOM**2 / df.t**4 + H_NOM**2 / df.w**4\n",
    "        )\n",
    "    )\n",
    "out_disp = [\"disp\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q3.1 Assemble and inspect the model\n",
    "\n",
    "Assemble a Grama model with the quantities above. Study the sinew plot below and answer the questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Assemble the Grama model for the cantilever beam\n",
    "###\n",
    "\n",
    "md_cantilever = (\n",
    "## TASK: Implement this model\n",
    "## NOTE: Make sure to use gr.cp_vec_function()\n",
    "\n",
    "# NOTE: Use these bounds in your optimization\n",
    "    >> gr.cp_bounds(\n",
    "        w=(4, 30),\n",
    "        t=(4, 30),\n",
    "    )\n",
    ")\n",
    "\n",
    "# NOTE: This will print out a summary of your model\n",
    "md_cantilever.printpretty()\n",
    "\n",
    "# NOTE: This will produce the sinew plot\n",
    "(\n",
    "    md_cantilever\n",
    "    >> gr.ev_sinews(df_det=\"swp\", n_sweeps=10, n_density=20, seed=101)\n",
    "    >> gr.pt_auto()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: Remember that each panel above shows a single output against a single input variable, with all other variables held *constant*.\n",
    "\n",
    "*Observe*:\n",
    "\n",
    "- Which variables have a *positive* impact on the following output? Which have a *negative* impact? (Note that I mean *positive* in sign of effect, not in terms of good.)\n",
    "  - `disp`: Positive impact: \n",
    "  - `disp`: Negative impact: \n",
    "  - `vol`: Positive impact: \n",
    "  - `vol`: Negative impact: \n",
    "  - `stress`: Positive impact: \n",
    "  - `stress`: Negative impact: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q3.2 Scalarize the objectives\n",
    "\n",
    "Complete the code below to scalarize the two objectives with the weight method. Use the weights provided in `w_vol, w_disp`.\n",
    "\n",
    "*Note*: Even when you get this right, the code will take a few moments to finish executing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Add another function to the model to scalarize\n",
    "#       the objectives `vol` and `disp`; use the\n",
    "#       weights w_vol and w_disp\n",
    "###\n",
    "\n",
    "# NOTE: No need to edit the scaffold code; just add at the TASK below\n",
    "\n",
    "# Set the weights for scalarization\n",
    "w_vol = 1e-4\n",
    "w_disp_all = np.logspace(-1, +1, 10)\n",
    "\n",
    "df_all = pd.DataFrame()\n",
    "# Iterate over all displacement weights\n",
    "for w_disp in w_disp_all:\n",
    "    md_tmp = (\n",
    "        md_cantilever\n",
    "# TASK: Add another function to the model to scalarize\n",
    "        \n",
    "    )\n",
    "    \n",
    "    # NOTE: The following code minimizes your scalarized\n",
    "    #       objective\n",
    "    df_res = (\n",
    "        md_tmp\n",
    "        >> gr.ev_min(\n",
    "            out_min=\"obj\",\n",
    "            out_geq=[\"stress_diff\"],\n",
    "        )\n",
    "        >> gr.tf_mutate(\n",
    "            pareto=gr.pareto_min(DF.disp, DF.vol),\n",
    "            w_disp=w_disp,\n",
    "        )\n",
    "    )\n",
    "    # Store the results\n",
    "    df_all = pd.concat((df_all, df_res), axis=0)\n",
    "# Reset the dataframe index\n",
    "df_all.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q3.3 Visualize and interpret\n",
    "\n",
    "Visualize the data `df_all` from above with the variables Tip Displacement and Volume. Answer the questions below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Visualize your Pareto frontier results; analyze the results\n",
    "###\n",
    "\n",
    "# NOTE: Add your results to the following figure\n",
    "plt.figure()\n",
    "plt.plot(40000, 1.0, \"rx\")\n",
    "# TASK: Visualize the results in df_all\n",
    "\n",
    "\n",
    "plt.xlabel(\"Volume (in^3)\")\n",
    "plt.ylabel(\"Tip Displacement (in)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observe*:\n",
    "\n",
    "- Qualitatively describe the tradeoff: Both Tip Displacement (D) and Volume (V) are to be minimized, but what shape does the frontier have?\n",
    "  - (Your response here)\n",
    "- At which end of the Pareto frontier do points tend to concentrate? What about the optimization problem can explain this phenomenon?\n",
    "  - (Your response here)\n",
    "- Suppose a customer currently uses a beam sizing at the red `X` pictured above. Based on your Pareto frontier results, what options for improvement would you provide to the customer?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project\n",
    "\n",
    "---\n",
    "\n",
    "### __Project Task__\n",
    "\n",
    "Look back on your project idea from the previous assignment. As we saw in this assignment, many problems have multiple objectives. This project task will help you connect these ideas to your project.\n",
    "\n",
    "*Task*: Provide an answer to each point below:\n",
    "\n",
    "- *Stakeholder Concerns*: Last assignment you wrote down your stakeholders' concerns; copy those over here for your reference.\n",
    "  - (Your response here)\n",
    "- *Multiple objectives*: Are there multiple objectives among your stakeholders' concerns?\n",
    "  - (Your response here)\n",
    "- *Correlated objectives*: Do you have any reason to believe that any of your objectives are *positively correlated*? \n",
    "  - (Your response here)\n",
    "  - *Note*: If two objectives are strongly positively correlated, then we need not consider both objectives. We can optimize one, and the other will be optimized \"for free\".\n",
    "- *Natural tradeoffs*: Do you have any reason to believe that any of your objectives exhibit *tradeoffs*? Do you have a physical reason to believe this?\n",
    "  - (Your response here)\n",
    "  - *Note*: If two (or more) objectives exhibit a tradeoff, then you'll need to consider all the objectives in your project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "---\n",
    "\n",
    "- del Rosario *et al.*, \"Assessing the frontier: Active learning, model accuracy, and multi-objective candidate discovery and optimization\" (2020) *J. Chem. Phys.*, [link](https://aip.scitation.org/doi/full/10.1063/5.0006124)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
